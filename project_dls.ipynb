{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx8C015cXRDO",
        "outputId": "0692cd3e-2f06-433e-8b7a-d2c88e948af2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "# Configuration\n",
        "AUDIO_DIR =  \"/content/drive/My Drive/fma_small\"\n",
        "METADATA_PATH = '/content/drive/My Drive/fma_metadata/tracks.csv'\n",
        "EMBEDDING_DIM = 768  # Wav2Vec base model\n",
        "CHECKPOINT_DIR = \"/content/drive/My Drive/\"\n",
        "EMBEDDING_DIM = 768\n",
        "SAMPLE_RATE = 16000\n",
        "CHUNK_SIZE = 5\n",
        "\n",
        "# 1. Load FMA Metadata\n",
        "def load_fma_metadata():\n",
        "    # Load the tracks.csv with proper multi-level header\n",
        "    tracks = pd.read_csv(METADATA_PATH, index_col=0, header=[0, 1])\n",
        "\n",
        "    tracks.columns = ['_'.join(col) for col in tracks.columns]\n",
        "\n",
        "    small = tracks[tracks['set_subset'] == 'small']\n",
        "    print(small.shape)\n",
        "\n",
        "    metadata = small[[\n",
        "        'track_title',\n",
        "        'artist_name',\n",
        "        'album_title',\n",
        "        'album_id'\n",
        "    ]]\n",
        "\n",
        "    return metadata\n",
        "\n",
        "metadata = load_fma_metadata()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzzBa6FwX2Fh",
        "outputId": "8de4a6df-b862-4bc1-e050-23cbbada3ef8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8000, 52)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Initialize Wav2Vec\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n",
        "model.share_memory()"
      ],
      "metadata": {
        "id": "5drA5Kz9ZhPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_audio_path(track_id):\n",
        "    tid_str = '{:06d}'.format(track_id)\n",
        "    return os.path.join(AUDIO_DIR, tid_str[:3], tid_str + '.mp3')\n",
        "\n",
        "def audio_to_embedding(audio_path):\n",
        "    try:\n",
        "        waveform, sr = torchaudio.load(audio_path)\n",
        "        if sr != SAMPLE_RATE:\n",
        "            resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Ensure mono audio by taking mean if multiple channels exist\n",
        "        if waveform.dim() > 1 and waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Ensure we have exactly CHUNK_SIZE seconds\n",
        "        target_samples = SAMPLE_RATE * CHUNK_SIZE\n",
        "        if waveform.shape[1] < target_samples:\n",
        "            padding = target_samples - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "        else:\n",
        "            waveform = waveform[:, :target_samples]\n",
        "\n",
        "        # Normalize audio\n",
        "        waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-8)\n",
        "\n",
        "        inputs = feature_extractor(\n",
        "            waveform.squeeze().numpy(),\n",
        "            sampling_rate=SAMPLE_RATE,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Ensure consistent embedding size\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "        if embedding.ndim == 0:  # handle scalar case\n",
        "            embedding = np.array([embedding])\n",
        "        return embedding\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def build_embedding_database():\n",
        "    # Initialize with empty array of correct shape if file doesn't exist\n",
        "    embeddings = []\n",
        "    try:\n",
        "        existing_embeddings = np.load('/content/drive/My Drive/embeddings.npy', allow_pickle=True)\n",
        "        if existing_embeddings.size > 0:\n",
        "            embeddings = list(existing_embeddings)\n",
        "    except (FileNotFoundError, ValueError):\n",
        "        pass\n",
        "\n",
        "    valid_tracks = []\n",
        "    try:\n",
        "        with open(\"valid_tracks.txt\", \"r\") as f:\n",
        "            valid_tracks = [int(line.strip()) for line in f]\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "    cnt = 0\n",
        "    for track_id, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
        "        if track_id in valid_tracks:  # Skip already processed tracks\n",
        "            continue\n",
        "\n",
        "        audio_path = get_audio_path(track_id)\n",
        "\n",
        "        if not os.path.exists(audio_path):\n",
        "            print(f\"Missing audio file: {audio_path}\")\n",
        "            continue\n",
        "\n",
        "        emb = audio_to_embedding(audio_path)\n",
        "        if emb is not None:\n",
        "            embeddings.append(emb)\n",
        "            valid_tracks.append(track_id)\n",
        "\n",
        "        cnt += 1\n",
        "        if cnt % 5 == 0:\n",
        "            np.save('/content/drive/My Drive/embeddings.npy', np.vstack(embeddings))\n",
        "            with open(\"valid_tracks.txt\", \"w\") as f:\n",
        "                f.write(\"\\n\".join(map(str, valid_tracks)))\n",
        "\n",
        "    if embeddings:\n",
        "        np.save('/content/drive/My Drive/embeddings.npy', np.vstack(embeddings))\n",
        "        with open(\"valid_tracks.txt\", \"w\") as f:\n",
        "            f.write(\"\\n\".join(map(str, valid_tracks)))\n",
        "\n",
        "    return np.vstack(embeddings) if embeddings else np.array([]), valid_tracks"
      ],
      "metadata": {
        "id": "QMRB9E66Zwlp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building embedding database\")\n",
        "embeddings, track_ids = build_embedding_database()\n",
        "print(f\"Generated {len(embeddings)} embeddings\")\n",
        "\n",
        "nn_model = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
        "nn_model.fit(embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "TNbkfI0UZzLc",
        "outputId": "f9e5e7e4-d00e-404b-ff32-f0cff2a55965"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building embedding database...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 4471/8000 [2:44:06<1:29:23,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/fma_small/099/099134.mp3: Failed to open the input \"/content/drive/My Drive/fma_small/099/099134.mp3\" (Invalid argument).\n",
            "Exception raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\n",
            "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7decb8d6c1b6 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
            "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7decb8d15a76 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
            "frame #2: <unknown function> + 0x42034 (0x7decfc669034 in /usr/local/lib/python3.11/dist-packages/torio/lib/libtorio_ffmpeg4.so)\n",
            "frame #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7decfc66ba34 in /usr/local/lib/python3.11/dist-packages/torio/lib/libtorio_ffmpeg4.so)\n",
            "frame #4: <unknown function> + 0x3bfee (0x7debe319afee in /usr/local/lib/python3.11/dist-packages/torio/lib/_torio_ffmpeg4.so)\n",
            "frame #5: <unknown function> + 0x330c7 (0x7debe31920c7 in /usr/local/lib/python3.11/dist-packages/torio/lib/_torio_ffmpeg4.so)\n",
            "frame #6: /usr/bin/python3() [0x55577b]\n",
            "frame #7: _PyObject_MakeTpCall + 0x27c (0x52f85c in /usr/bin/python3)\n",
            "frame #8: /usr/bin/python3() [0x5855cd]\n",
            "frame #9: /usr/bin/python3() [0x56e459]\n",
            "frame #10: /usr/bin/python3() [0x52fc40]\n",
            "frame #11: <unknown function> + 0xfc6b (0x7decfc6edc6b in /usr/local/lib/python3.11/dist-packages/torchaudio/lib/_torchaudio.so)\n",
            "frame #12: _PyObject_MakeTpCall + 0x27c (0x52f85c in /usr/bin/python3)\n",
            "frame #13: _PyEval_EvalFrameDefault + 0x6bc (0x53da0c in /usr/bin/python3)\n",
            "frame #14: _PyFunction_Vectorcall + 0x173 (0x5663a3 in /usr/bin/python3)\n",
            "frame #15: /usr/bin/python3() [0x56e0e6]\n",
            "frame #16: _PyObject_MakeTpCall + 0x23b (0x52f81b in /usr/bin/python3)\n",
            "frame #17: _PyEval_EvalFrameDefault + 0x6bc (0x53da0c in /usr/bin/python3)\n",
            "frame #18: /usr/bin/python3() [0x613cd4]\n",
            "frame #19: PyEval_EvalCode + 0x97 (0x613337 in /usr/bin/python3)\n",
            "frame #20: /usr/bin/python3() [0x62d0d3]\n",
            "frame #21: _PyEval_EvalFrameDefault + 0x38f4 (0x540c44 in /usr/bin/python3)\n",
            "frame #22: /usr/bin/python3() [0x628b90]\n",
            "frame #23: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #24: /usr/bin/python3() [0x628b90]\n",
            "frame #25: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #26: /usr/bin/python3() [0x628b90]\n",
            "frame #27: /usr/bin/python3() [0x62b1ac]\n",
            "frame #28: _PyEval_EvalFrameDefault + 0x3a82 (0x540dd2 in /usr/bin/python3)\n",
            "frame #29: /usr/bin/python3() [0x585ce7]\n",
            "frame #30: /usr/bin/python3() [0x5854ce]\n",
            "frame #31: PyObject_Call + 0xf4 (0x570914 in /usr/bin/python3)\n",
            "frame #32: _PyEval_EvalFrameDefault + 0x491a (0x541c6a in /usr/bin/python3)\n",
            "frame #33: /usr/bin/python3() [0x628b90]\n",
            "frame #34: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #35: /usr/bin/python3() [0x628b90]\n",
            "frame #36: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #37: /usr/bin/python3() [0x628b90]\n",
            "frame #38: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #39: /usr/bin/python3() [0x628b90]\n",
            "frame #40: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #41: /usr/bin/python3() [0x628b90]\n",
            "frame #42: <unknown function> + 0x745f (0x7ded0833845f in /usr/lib/python3.11/lib-dynload/_asyncio.cpython-311-x86_64-linux-gnu.so)\n",
            "frame #43: /usr/bin/python3() [0x553bdf]\n",
            "frame #44: /usr/bin/python3() [0x4d0da1]\n",
            "frame #45: /usr/bin/python3() [0x4e9708]\n",
            "frame #46: /usr/bin/python3() [0x54b41b]\n",
            "frame #47: _PyEval_EvalFrameDefault + 0x978c (0x546adc in /usr/bin/python3)\n",
            "frame #48: /usr/bin/python3() [0x613cd4]\n",
            "frame #49: PyEval_EvalCode + 0x97 (0x613337 in /usr/bin/python3)\n",
            "frame #50: /usr/bin/python3() [0x62d0d3]\n",
            "frame #51: /usr/bin/python3() [0x54b41b]\n",
            "frame #52: PyObject_Vectorcall + 0x35 (0x54b305 in /usr/bin/python3)\n",
            "frame #53: _PyEval_EvalFrameDefault + 0x6bc (0x53da0c in /usr/bin/python3)\n",
            "frame #54: _PyFunction_Vectorcall + 0x173 (0x5663a3 in /usr/bin/python3)\n",
            "frame #55: /usr/bin/python3() [0x63ef00]\n",
            "frame #56: Py_RunMain + 0x13c (0x63e85c in /usr/bin/python3)\n",
            "frame #57: Py_BytesMain + 0x2d (0x6045dd in /usr/bin/python3)\n",
            "frame #58: <unknown function> + 0x29d90 (0x7ded08a75d90 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "frame #59: __libc_start_main + 0x80 (0x7ded08a75e40 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "frame #60: _start + 0x25 (0x604465 in /usr/bin/python3)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████▏   | 4904/8000 [2:59:50<1:15:26,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/fma_small/108/108925.mp3: Failed to open the input \"/content/drive/My Drive/fma_small/108/108925.mp3\" (Invalid argument).\n",
            "Exception raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\n",
            "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7decb8d6c1b6 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
            "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7decb8d15a76 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
            "frame #2: <unknown function> + 0x42034 (0x7decfc669034 in /usr/local/lib/python3.11/dist-packages/torio/lib/libtorio_ffmpeg4.so)\n",
            "frame #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7decfc66ba34 in /usr/local/lib/python3.11/dist-packages/torio/lib/libtorio_ffmpeg4.so)\n",
            "frame #4: <unknown function> + 0x3bfee (0x7debe319afee in /usr/local/lib/python3.11/dist-packages/torio/lib/_torio_ffmpeg4.so)\n",
            "frame #5: <unknown function> + 0x330c7 (0x7debe31920c7 in /usr/local/lib/python3.11/dist-packages/torio/lib/_torio_ffmpeg4.so)\n",
            "frame #6: /usr/bin/python3() [0x55577b]\n",
            "frame #7: _PyObject_MakeTpCall + 0x27c (0x52f85c in /usr/bin/python3)\n",
            "frame #8: /usr/bin/python3() [0x5855cd]\n",
            "frame #9: /usr/bin/python3() [0x56e459]\n",
            "frame #10: /usr/bin/python3() [0x52fc40]\n",
            "frame #11: <unknown function> + 0xfc6b (0x7decfc6edc6b in /usr/local/lib/python3.11/dist-packages/torchaudio/lib/_torchaudio.so)\n",
            "frame #12: _PyObject_MakeTpCall + 0x27c (0x52f85c in /usr/bin/python3)\n",
            "frame #13: _PyEval_EvalFrameDefault + 0x6bc (0x53da0c in /usr/bin/python3)\n",
            "frame #14: _PyFunction_Vectorcall + 0x173 (0x5663a3 in /usr/bin/python3)\n",
            "frame #15: /usr/bin/python3() [0x56e0e6]\n",
            "frame #16: _PyObject_MakeTpCall + 0x23b (0x52f81b in /usr/bin/python3)\n",
            "frame #17: _PyEval_EvalFrameDefault + 0x6bc (0x53da0c in /usr/bin/python3)\n",
            "frame #18: /usr/bin/python3() [0x613cd4]\n",
            "frame #19: PyEval_EvalCode + 0x97 (0x613337 in /usr/bin/python3)\n",
            "frame #20: /usr/bin/python3() [0x62d0d3]\n",
            "frame #21: _PyEval_EvalFrameDefault + 0x38f4 (0x540c44 in /usr/bin/python3)\n",
            "frame #22: /usr/bin/python3() [0x628b90]\n",
            "frame #23: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #24: /usr/bin/python3() [0x628b90]\n",
            "frame #25: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #26: /usr/bin/python3() [0x628b90]\n",
            "frame #27: /usr/bin/python3() [0x62b1ac]\n",
            "frame #28: _PyEval_EvalFrameDefault + 0x3a82 (0x540dd2 in /usr/bin/python3)\n",
            "frame #29: /usr/bin/python3() [0x585ce7]\n",
            "frame #30: /usr/bin/python3() [0x5854ce]\n",
            "frame #31: PyObject_Call + 0xf4 (0x570914 in /usr/bin/python3)\n",
            "frame #32: _PyEval_EvalFrameDefault + 0x491a (0x541c6a in /usr/bin/python3)\n",
            "frame #33: /usr/bin/python3() [0x628b90]\n",
            "frame #34: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #35: /usr/bin/python3() [0x628b90]\n",
            "frame #36: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #37: /usr/bin/python3() [0x628b90]\n",
            "frame #38: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #39: /usr/bin/python3() [0x628b90]\n",
            "frame #40: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #41: /usr/bin/python3() [0x628b90]\n",
            "frame #42: <unknown function> + 0x745f (0x7ded0833845f in /usr/lib/python3.11/lib-dynload/_asyncio.cpython-311-x86_64-linux-gnu.so)\n",
            "frame #43: /usr/bin/python3() [0x553bdf]\n",
            "frame #44: /usr/bin/python3() [0x4d0da1]\n",
            "frame #45: /usr/bin/python3() [0x4e9708]\n",
            "frame #46: /usr/bin/python3() [0x54b41b]\n",
            "frame #47: _PyEval_EvalFrameDefault + 0x978c (0x546adc in /usr/bin/python3)\n",
            "frame #48: /usr/bin/python3() [0x613cd4]\n",
            "frame #49: PyEval_EvalCode + 0x97 (0x613337 in /usr/bin/python3)\n",
            "frame #50: /usr/bin/python3() [0x62d0d3]\n",
            "frame #51: /usr/bin/python3() [0x54b41b]\n",
            "frame #52: PyObject_Vectorcall + 0x35 (0x54b305 in /usr/bin/python3)\n",
            "frame #53: _PyEval_EvalFrameDefault + 0x6bc (0x53da0c in /usr/bin/python3)\n",
            "frame #54: _PyFunction_Vectorcall + 0x173 (0x5663a3 in /usr/bin/python3)\n",
            "frame #55: /usr/bin/python3() [0x63ef00]\n",
            "frame #56: Py_RunMain + 0x13c (0x63e85c in /usr/bin/python3)\n",
            "frame #57: Py_BytesMain + 0x2d (0x6045dd in /usr/bin/python3)\n",
            "frame #58: <unknown function> + 0x29d90 (0x7ded08a75d90 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "frame #59: __libc_start_main + 0x80 (0x7ded08a75e40 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "frame #60: _start + 0x25 (0x604465 in /usr/bin/python3)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 6966/8000 [4:15:48<26:22,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/My Drive/fma_small/133/133297.mp3: Failed to open the input \"/content/drive/My Drive/fma_small/133/133297.mp3\" (Invalid argument).\n",
            "Exception raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\n",
            "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7decb8d6c1b6 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
            "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7decb8d15a76 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
            "frame #2: <unknown function> + 0x42034 (0x7decfc669034 in /usr/local/lib/python3.11/dist-packages/torio/lib/libtorio_ffmpeg4.so)\n",
            "frame #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7decfc66ba34 in /usr/local/lib/python3.11/dist-packages/torio/lib/libtorio_ffmpeg4.so)\n",
            "frame #4: <unknown function> + 0x3bfee (0x7debe319afee in /usr/local/lib/python3.11/dist-packages/torio/lib/_torio_ffmpeg4.so)\n",
            "frame #5: <unknown function> + 0x330c7 (0x7debe31920c7 in /usr/local/lib/python3.11/dist-packages/torio/lib/_torio_ffmpeg4.so)\n",
            "frame #6: /usr/bin/python3() [0x55577b]\n",
            "frame #7: _PyObject_MakeTpCall + 0x27c (0x52f85c in /usr/bin/python3)\n",
            "frame #8: /usr/bin/python3() [0x5855cd]\n",
            "frame #9: /usr/bin/python3() [0x56e459]\n",
            "frame #10: /usr/bin/python3() [0x52fc40]\n",
            "frame #11: <unknown function> + 0xfc6b (0x7decfc6edc6b in /usr/local/lib/python3.11/dist-packages/torchaudio/lib/_torchaudio.so)\n",
            "frame #12: _PyObject_MakeTpCall + 0x27c (0x52f85c in /usr/bin/python3)\n",
            "frame #13: _PyEval_EvalFrameDefault + 0x6bc (0x53da0c in /usr/bin/python3)\n",
            "frame #14: _PyFunction_Vectorcall + 0x173 (0x5663a3 in /usr/bin/python3)\n",
            "frame #15: /usr/bin/python3() [0x56e0e6]\n",
            "frame #16: _PyObject_MakeTpCall + 0x23b (0x52f81b in /usr/bin/python3)\n",
            "frame #17: _PyEval_EvalFrameDefault + 0x6bc (0x53da0c in /usr/bin/python3)\n",
            "frame #18: /usr/bin/python3() [0x613cd4]\n",
            "frame #19: PyEval_EvalCode + 0x97 (0x613337 in /usr/bin/python3)\n",
            "frame #20: /usr/bin/python3() [0x62d0d3]\n",
            "frame #21: _PyEval_EvalFrameDefault + 0x38f4 (0x540c44 in /usr/bin/python3)\n",
            "frame #22: /usr/bin/python3() [0x628b90]\n",
            "frame #23: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #24: /usr/bin/python3() [0x628b90]\n",
            "frame #25: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #26: /usr/bin/python3() [0x628b90]\n",
            "frame #27: /usr/bin/python3() [0x62b1ac]\n",
            "frame #28: _PyEval_EvalFrameDefault + 0x3a82 (0x540dd2 in /usr/bin/python3)\n",
            "frame #29: /usr/bin/python3() [0x585ce7]\n",
            "frame #30: /usr/bin/python3() [0x5854ce]\n",
            "frame #31: PyObject_Call + 0xf4 (0x570914 in /usr/bin/python3)\n",
            "frame #32: _PyEval_EvalFrameDefault + 0x491a (0x541c6a in /usr/bin/python3)\n",
            "frame #33: /usr/bin/python3() [0x628b90]\n",
            "frame #34: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #35: /usr/bin/python3() [0x628b90]\n",
            "frame #36: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #37: /usr/bin/python3() [0x628b90]\n",
            "frame #38: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #39: /usr/bin/python3() [0x628b90]\n",
            "frame #40: _PyEval_EvalFrameDefault + 0x343f (0x54078f in /usr/bin/python3)\n",
            "frame #41: /usr/bin/python3() [0x628b90]\n",
            "frame #42: <unknown function> + 0x745f (0x7ded0833845f in /usr/lib/python3.11/lib-dynload/_asyncio.cpython-311-x86_64-linux-gnu.so)\n",
            "frame #43: /usr/bin/python3() [0x553bdf]\n",
            "frame #44: /usr/bin/python3() [0x4d0da1]\n",
            "frame #45: /usr/bin/python3() [0x4e9708]\n",
            "frame #46: /usr/bin/python3() [0x54b41b]\n",
            "frame #47: _PyEval_EvalFrameDefault + 0x978c (0x546adc in /usr/bin/python3)\n",
            "frame #48: /usr/bin/python3() [0x613cd4]\n",
            "frame #49: PyEval_EvalCode + 0x97 (0x613337 in /usr/bin/python3)\n",
            "frame #50: /usr/bin/python3() [0x62d0d3]\n",
            "frame #51: /usr/bin/python3() [0x54b41b]\n",
            "frame #52: PyObject_Vectorcall + 0x35 (0x54b305 in /usr/bin/python3)\n",
            "frame #53: _PyEval_EvalFrameDefault + 0x6bc (0x53da0c in /usr/bin/python3)\n",
            "frame #54: _PyFunction_Vectorcall + 0x173 (0x5663a3 in /usr/bin/python3)\n",
            "frame #55: /usr/bin/python3() [0x63ef00]\n",
            "frame #56: Py_RunMain + 0x13c (0x63e85c in /usr/bin/python3)\n",
            "frame #57: Py_BytesMain + 0x2d (0x6045dd in /usr/bin/python3)\n",
            "frame #58: <unknown function> + 0x29d90 (0x7ded08a75d90 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "frame #59: __libc_start_main + 0x80 (0x7ded08a75e40 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "frame #60: _start + 0x25 (0x604465 in /usr/bin/python3)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8000/8000 [4:54:15<00:00,  2.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 8027 embeddings\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(metric='cosine')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>NearestNeighbors</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neighbors.NearestNeighbors.html\">?<span>Documentation for NearestNeighbors</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "class MusicSimilaritySearchANNPQ:\n",
        "    def __init__(self, metadata, embeddings, track_ids, audio_dir=AUDIO_DIR, n_neighbors=5):\n",
        "        self.metadata = metadata\n",
        "        self.audio_dir = audio_dir\n",
        "        self.dim = embeddings.shape[1]\n",
        "        self.track_ids = track_ids  # Keep list of track IDs aligned to embeddings\n",
        "\n",
        "        # Normalize embeddings\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.embeddings = embeddings\n",
        "\n",
        "        # Build FAISS index\n",
        "        self.index = faiss.IndexPQ(self.dim, 8, 8)\n",
        "        self.index.train(embeddings)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "    def get_similar_tracks(self, query_embedding, k=5):\n",
        "        query_embedding = query_embedding.astype('float32')\n",
        "        faiss.normalize_L2(query_embedding.reshape(1, -1))\n",
        "\n",
        "        distances, indices = self.index.search(query_embedding.reshape(1, -1), k)\n",
        "        similar_tracks = []\n",
        "        for i in range(k):\n",
        "            idx = indices[0][i]\n",
        "            if idx >= len(self.track_ids):\n",
        "                continue  # Avoid out-of-bounds\n",
        "\n",
        "            track_id = self.track_ids[idx]\n",
        "            if track_id not in self.metadata.index:\n",
        "                continue  # Skip if metadata is missing\n",
        "\n",
        "            row = self.metadata.loc[track_id]\n",
        "            track_info = {\n",
        "                'track_id': track_id,\n",
        "                'title': row['track_title'],\n",
        "                'artist': row['artist_name'],\n",
        "                'album': row['album_title'],\n",
        "                'distance': 1 - distances[0][i],  # similarity = 1 - distance\n",
        "                'audio_path': get_audio_path(track_id)\n",
        "            }\n",
        "            similar_tracks.append(track_info)\n",
        "        return similar_tracks\n",
        "\n",
        "    def search_by_audio(self, input_mp3_path, k=5):\n",
        "        query_embedding = audio_to_embedding(input_mp3_path)\n",
        "        if query_embedding is None:\n",
        "            return []\n",
        "        return self.get_similar_tracks(query_embedding, k)\n",
        "\n",
        "def load_embeddings():\n",
        "    embedding_file = os.path.join(CHECKPOINT_DIR, 'embeddings.npy')\n",
        "    valid_tracks_file = os.path.join(CHECKPOINT_DIR, 'valid_tracks.txt')\n",
        "    print(embedding_file, valid_tracks_file)\n",
        "\n",
        "    print(\"Loading pre-computed embeddings...\")\n",
        "    embeddings = np.load(embedding_file).astype('float32')  # Ensure float32\n",
        "    with open(valid_tracks_file, 'r') as f:\n",
        "        valid_tracks = [int(line.strip()) for line in f]\n",
        "    valid_metadata = metadata[metadata.index.isin(valid_tracks)]\n",
        "    return valid_metadata, embeddings\n",
        ""
      ],
      "metadata": {
        "id": "wrryshmiEf03"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_metadata, embeddings = load_embeddings()\n",
        "\n",
        "with open(os.path.join(CHECKPOINT_DIR, 'valid_tracks.txt'), 'r') as f:\n",
        "    track_ids = [int(line.strip()) for line in f]\n",
        "\n",
        "valid_metadata = metadata[metadata.index.isin(track_ids)]\n",
        "\n",
        "music_search = MusicSimilaritySearchANNPQ(valid_metadata, embeddings, track_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzsTEp6IEsJ3",
        "outputId": "43513ee8-6ddb-4226-d07b-fd644f332da7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/embeddings.npy /content/drive/My Drive/valid_tracks.txt\n",
            "Loading pre-computed embeddings...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_mp3 = \"006331.mp3\"\n",
        "\n",
        "if os.path.exists(input_mp3):\n",
        "    print(f\"\\nFinding similar tracks for: {input_mp3}\")\n",
        "    similar_tracks = music_search.search_by_audio(input_mp3)\n",
        "\n",
        "    for i, track in enumerate(similar_tracks, 1):\n",
        "        print(f\"\\nSimilar track #{i}:\")\n",
        "        print(f\"Title: {track['title']}\")\n",
        "        print(f\"Artist: {track['artist']}\")\n",
        "        print(f\"Album: {track['album']}\")\n",
        "        print(f\"Similarity: {1 - track['distance']:.3f}\")\n",
        "        print(f\"Audio path: {track['audio_path']}\")\n",
        "else:\n",
        "    print(f\"Input file not found: {input_mp3}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N0DtOE6I28C",
        "outputId": "615b2a2e-aa49-41c5-cf2c-433ff39a2404"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finding similar tracks for: 006331.mp3\n",
            "\n",
            "Similar track #1:\n",
            "Title: Crossover\n",
            "Artist: EPMD\n",
            "Album: Live at ATP 2008\n",
            "Similarity: 0.071\n",
            "Audio path: /content/drive/My Drive/fma_small/006/006439.mp3\n",
            "\n",
            "Similar track #2:\n",
            "Title: one last try\n",
            "Artist: Chicken Jones\n",
            "Album: Sour Soul 12\n",
            "Similarity: 0.079\n",
            "Audio path: /content/drive/My Drive/fma_small/043/043867.mp3\n",
            "\n",
            "Similar track #3:\n",
            "Title: Ciganka Je Malena\n",
            "Artist: Gogofski\n",
            "Album: Live at the 2016 Golden Festival\n",
            "Similarity: 0.085\n",
            "Audio path: /content/drive/My Drive/fma_small/132/132139.mp3\n",
            "\n",
            "Similar track #4:\n",
            "Title: Success\n",
            "Artist: K. Sparks\n",
            "Album: Diagnosis: Success\n",
            "Similarity: 0.085\n",
            "Audio path: /content/drive/My Drive/fma_small/054/054365.mp3\n",
            "\n",
            "Similar track #5:\n",
            "Title: Enyi Wana Damu  Sharmila and Black Star Orchestra\n",
            "Artist: The Sounds of Taraab\n",
            "Album: Live at WFMU on Rob Weisberg's Show on 4/7/2007\n",
            "Similarity: 0.085\n",
            "Audio path: /content/drive/My Drive/fma_small/004/004236.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def evaluate_model(music_search, valid_metadata, embeddings, valid_track_ids, k=5, sim_threshold=0.85):\n",
        "    top_k_hits = 0\n",
        "    reciprocal_ranks = []\n",
        "    total_queries = 0\n",
        "    total_latency = 0\n",
        "    total_fp = 0\n",
        "    total_sim_scores = []\n",
        "    simulated_clicks = 0\n",
        "    session_lengths = []\n",
        "\n",
        "    for i, query_id in enumerate(valid_track_ids):\n",
        "        query_path = get_audio_path(query_id)\n",
        "        if not os.path.exists(query_path):\n",
        "            continue\n",
        "\n",
        "        start_time = time.time()\n",
        "        query_embedding = audio_to_embedding(query_path)\n",
        "        if query_embedding is None:\n",
        "            continue\n",
        "\n",
        "        results = music_search.get_similar_tracks(query_embedding, k)\n",
        "        latency = time.time() - start_time\n",
        "        total_latency += latency\n",
        "\n",
        "        total_queries += 1\n",
        "        correct_found = False\n",
        "        session_length = 0\n",
        "\n",
        "        for rank, result in enumerate(results, 1):\n",
        "            retrieved_id = result['track_id']\n",
        "            sim = 1 - result['distance']\n",
        "            total_sim_scores.append(sim)\n",
        "            session_length += 1\n",
        "\n",
        "            if retrieved_id == query_id:\n",
        "                top_k_hits += 1\n",
        "                reciprocal_ranks.append(1 / rank)\n",
        "                correct_found = True\n",
        "\n",
        "            elif sim > sim_threshold:\n",
        "                simulated_clicks += 1\n",
        "            else:\n",
        "                total_fp += 1\n",
        "\n",
        "        if not correct_found:\n",
        "            reciprocal_ranks.append(0)\n",
        "\n",
        "        session_lengths.append(session_length if correct_found else 1)\n",
        "\n",
        "    top_k_accuracy = top_k_hits / total_queries\n",
        "    mrr = sum(reciprocal_ranks) / total_queries\n",
        "    avg_cos_sim = sum(total_sim_scores) / len(total_sim_scores)\n",
        "    false_positive_rate = total_fp / (total_queries * k)\n",
        "\n",
        "    avg_latency = total_latency / total_queries\n",
        "    avg_session_length = sum(session_lengths) / total_queries\n",
        "\n",
        "    return {\n",
        "        \"Top-5 Accuracy\": top_k_accuracy,\n",
        "        \"Mean Reciprocal Rank\": mrr,\n",
        "        \"Average Cosine Similarity\": avg_cos_sim,\n",
        "        \"False Positive Rate\": false_positive_rate,\n",
        "        \"Search Latency (s)\": avg_latency,\n",
        "        \"Simulated Session Length\": avg_session_length\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "cOFfrdoYVAfq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "track_ids = track_ids[:50]\n",
        "metrics = evaluate_model(music_search, valid_metadata, embeddings, track_ids)\n",
        "print(\"\\n--- Evaluation Results ---\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wp0FQztbeoT",
        "outputId": "7b8c3e0d-1588-4938-ede2-3f439e51932e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation Results ---\n",
            "Top-5 Accuracy: 0.0200\n",
            "Mean Reciprocal Rank: 0.0200\n",
            "Average Cosine Similarity: 0.1472\n",
            "False Positive Rate: 0.9960\n",
            "Search Latency (s): 1.6852\n",
            "Simulated Conversion Rate: 0.0000\n",
            "Simulated Session Length: 1.0800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "class MusicSimilaritySearchKDTree:\n",
        "    def __init__(self, metadata, embeddings, track_ids, audio_dir=AUDIO_DIR, n_neighbors=5):\n",
        "        self.metadata = metadata\n",
        "        self.audio_dir = audio_dir\n",
        "        self.embeddings = embeddings\n",
        "        self.track_ids = track_ids\n",
        "        self.nn_model = NearestNeighbors(n_neighbors=n_neighbors, algorithm='kd_tree', metric='euclidean')\n",
        "        self.nn_model.fit(embeddings)\n",
        "\n",
        "    def get_similar_tracks(self, query_embedding, k=5):\n",
        "        query_embedding = query_embedding.astype('float32').reshape(1, -1)\n",
        "        distances, indices = self.nn_model.kneighbors(query_embedding, n_neighbors=k)\n",
        "        similar_tracks = []\n",
        "        for i in range(k):\n",
        "            idx = indices[0][i]\n",
        "            if idx >= len(self.track_ids):\n",
        "                continue\n",
        "\n",
        "            track_id = self.track_ids[idx]\n",
        "            if track_id not in self.metadata.index:\n",
        "                continue\n",
        "\n",
        "            row = self.metadata.loc[track_id]\n",
        "            track_info = {\n",
        "                'track_id': track_id,\n",
        "                'title': row['track_title'],\n",
        "                'artist': row['artist_name'],\n",
        "                'album': row['album_title'],\n",
        "                'distance': distances[0][i],\n",
        "                'audio_path': get_audio_path(track_id)\n",
        "            }\n",
        "            similar_tracks.append(track_info)\n",
        "        return similar_tracks\n",
        "\n",
        "    def search_by_audio(self, input_mp3_path, k=5):\n",
        "        query_embedding = audio_to_embedding(input_mp3_path)\n",
        "        if query_embedding is None:\n",
        "            return []\n",
        "        return self.get_similar_tracks(query_embedding, k)\n"
      ],
      "metadata": {
        "id": "ruu3mmTKVAYJ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_metadata, embeddings = load_embeddings()\n",
        "\n",
        "with open(os.path.join(CHECKPOINT_DIR, 'valid_tracks.txt'), 'r') as f:\n",
        "    track_ids = [int(line.strip()) for line in f]\n",
        "\n",
        "valid_metadata = metadata[metadata.index.isin(track_ids)]\n",
        "\n",
        "music_search = MusicSimilaritySearchKDTree(valid_metadata, embeddings, track_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVyH6-nnbgsL",
        "outputId": "d2b2ba1c-4516-470c-a254-291499d9a171"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/embeddings.npy /content/drive/My Drive/valid_tracks.txt\n",
            "Loading pre-computed embeddings...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation\n",
        "track_ids = track_ids[:50]\n",
        "metrics = evaluate_model(music_search, valid_metadata, embeddings, track_ids)\n",
        "print(\"\\n--- Evaluation Results ---\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dADn6KiCbgP1",
        "outputId": "e3d28146-5ce2-43d0-b04e-7c23cce25e02"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation Results ---\n",
            "Top-5 Accuracy: 0.0200\n",
            "Mean Reciprocal Rank: 0.0100\n",
            "Average Cosine Similarity: -1.3704\n",
            "False Positive Rate: 0.9080\n",
            "Search Latency (s): 1.6754\n",
            "Simulated Conversion Rate: 0.0840\n",
            "Simulated Session Length: 1.0800\n"
          ]
        }
      ]
    }
  ]
}